services:
  # create a SuperLink service
  superlink:
    build:
      context: ${PROJECT_DIR:-../}
      dockerfile_inline: |
       FROM flwr/superlink:1.17.0

        USER root
        RUN apk add --no-cache build-base gcc g++

        USER app
        WORKDIR /app
        COPY --chown=app:app pyproject.toml .
        RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
           && python -m pip install -U --no-cache-dir .

        ENTRYPOINT ["flower-superlink"]
    container_name: SuperLink
    environment:
      - FLWR_LOG_LEVEL=ERROR
    command:
      - --insecure
    ports:
      - 9093:9093

  # create a ServerApp service
  
  # create two SuperNode services with different node configs
  supernode-1:
    build:
      context: ${PROJECT_DIR:-../}
      dockerfile_inline: |
       FROM flwr/supernode:1.17.0

        # gcc is required for the fastai quickstart example
        USER root
        RUN apk-get update \
            && apk-get -y --no-install-recommends install \
            build-essential \
            && rm -rf /var/lib/apk/lists/*
        USER app

        WORKDIR /app
        COPY --chown=app:app pyproject.toml .
        RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
           && python -m pip install -U .

        ENTRYPOINT ["flower-supernode"]
    volumes:
      - "../client_1/data:/app/data"  # Private Dataset client 1
    container_name: SuperNode-1
    command:
      - --insecure
      - --superlink
      - superlink:9092
      - --node-config
      - "partition-id=0 num-partitions=2"
    deploy:
      resources:
        limits:
          cpus: "16"
          memory: "2GB"
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]          
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    depends_on:
      - superlink
  # supernode 2, clientapp as subprocess
  supernode-2:
    build:
      context: ${PROJECT_DIR:-../}
      dockerfile_inline: |
       FROM flwr/supernode:1.17.0

        # gcc is required for the fastai quickstart example
        USER root
        RUN apk add --no-cache build-base gcc g++
        USER app

        WORKDIR /app
        COPY --chown=app:app pyproject.toml .
        RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
           && python -m pip install -U --no-cache-dir .

        ENTRYPOINT ["flower-supernode"]
    volumes:
      - "../client_2/data:/app/data"  # Private Dataset client 1
    container_name: SuperNode-2
    command:
      - --insecure
      - --superlink
      - superlink:9093
      - --node-config
      - "partition-id=0 num-partitions=2"
    deploy:
      resources:
        limits:
          cpus: "16"
          memory: "2GB"
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]          
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    depends_on:
      - superlink


  # uncomment to add another SuperNode
  #
  # supernode-3:
  #   image: flwr/supernode:${FLWR_VERSION:-1.15.2}
  #   command:
  #     - --insecure
  #     - --superlink
  #     - superlink:9092
  #     - --clientappio-api-address
  #     - 0.0.0.0:9096
  #     - --isolation
  #     - process
  #     - --node-config
  #     - "partition-id=1 num-partitions=2"
  #   depends_on:
  #     - superlink

  # create two ClientApp services
  # clientapp-1:
  #   build:
  #     context: ${PROJECT_DIR:-../}
  #     dockerfile_inline: |
  #      FROM theolo0204/raido:latest

  #       # gcc is required for the fastai quickstart example
  #       USER root
  #       RUN apt-get update \
  #           && apt-get -y --no-install-recommends install \
  #           build-essential \
  #           && rm -rf /var/lib/apt/lists/*
  #       USER app

  #       WORKDIR /app
  #       COPY --chown=app:app pyproject.toml .
  #       RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
  #          && python -m pip install -U --no-cache-dir .

  #       ENTRYPOINT ["flwr-clientapp"]
  #   volumes:
  #     - "../client_1/data:/app/data"  # Private Dataset client 1
  #   command:
  #     - --insecure
  #     - --clientappio-api-address
  #     - supernode-1:9094
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "16"
  #         memory: "2GB"
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   runtime: nvidia
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=all
  #   stop_signal: SIGINT
  #   container_name: clientApp-A
  #   depends_on:
  #     - supernode-1

  # clientapp-2:
  #   build:
  #     context: ${PROJECT_DIR:-../}

  #     dockerfile_inline: |
  #       FROM theolo0204/raido:latest


  #       # gcc is required for the fastai quickstart example
  #       USER root
  #       RUN apt-get update \
  #           && apt-get -y --no-install-recommends install \
  #           build-essential \
  #           && rm -rf /var/lib/apt/lists/*
  #       USER app

  #       WORKDIR /app
  #       COPY --chown=app:app pyproject.toml .
  #       RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
  #         && python -m pip install -U --no-cache-dir .

  #       ENTRYPOINT ["flwr-clientapp"]
  #   volumes:
  #     - "../client_2/data:/app/data"  # Private Dataset client 2
  #   command:
  #     - --insecure
  #     - --clientappio-api-address
  #     - supernode-2:9095
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "16"
  #         memory: "2GB"
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]          
  #   runtime: nvidia
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=all
  #   stop_signal: SIGINT
  #   container_name: clientApp-B
  #   depends_on:
  #     - supernode-2
  # uncomment to add another ClientApp
  #
  # clientapp-3:
  #   build:
  #     context: ${PROJECT_DIR:-.}
  #     dockerfile_inline: |
  #       FROM flwr/clientapp:${FLWR_VERSION:-1.15.2}

  #       # gcc is required for the fastai quickstart example
  #       USER root
  #       RUN apt-get update \
  #           && apt-get -y --no-install-recommends install \
  #           build-essential \
  #           && rm -rf /var/lib/apt/lists/*
  #       USER app

  #       WORKDIR /app
  #       COPY --chown=app:app pyproject.toml .
  #       RUN sed -i 's/.*flwr\[simulation\].*//' pyproject.toml \
  #         && python -m pip install -U --no-cache-dir .

  #       ENTRYPOINT ["flwr-clientapp"]
  #   command:
  #     - --insecure
  #     - --clientappio-api-address
  #     - supernode-3:9096
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2"
  #   stop_signal: SIGINT
  #   depends_on:
  #     - supernode-3

  master:
    build:
      context: ${PROJECT_DIR:-../master_service}  # Path to your master service code
      dockerfile_inline: |
        FROM python:3.11-slim

        # Set the working directory in the container
        WORKDIR /app

        # Copy requirements.txt and install dependencies
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt

        # Copy the rest of the app code into the container
        COPY . .
        # Expose Flask API port
        EXPOSE 5000

        # Set the command to run your Flask app
        CMD ["python", "master_app.py"]
        
    container_name: Master
    environment:
      - FLWR_LOG_LEVEL=ERROR
    ports:
      - "5000:5000"  # Expose Flask API to the host on port 5000
    volumes:
      - "../../flower-app:/app/flower-app"  # Mount the code directory to the container
    